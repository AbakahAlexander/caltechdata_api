import argparse, os, json
import s3fs
from datacite import schema43
from caltechdata_api import caltechdata_write

parser = argparse.ArgumentParser(
    description="Adds S3-stored pilot files and a DataCite 4.3 standard json record\
        to CaltechDATA repository"
)
parser.add_argument("folder", nargs=1, help="Folder")
parser.add_argument(
    "json_file", nargs=1, help="file name for json DataCite metadata file"
)

args = parser.parse_args()

# Get access token as environment variable
token = os.environ["TINDTOK"]

endpoint = "https://renc.osn.xsede.org/"

# Get metadata and files from bucket
s3 = s3fs.S3FileSystem(anon=True, client_kwargs={"endpoint_url": endpoint})


path = "ini210004tommorrell/" + args.folder[0] + "/"
records = s3.ls(path)
# Strip out reference to top level directory
repeat = records.pop(0)
assert repeat == path

abstract = """This record is a component of the Materials Experiment and
Analysis Database (MEAD). It contains raw data and metadata from millions 
of materials synthesis and characterization experiments, as well as the 
analysis and distillation of that data into property and performance 
metrics. The unprecedented quantity and diversity of experimental data 
are searchable by experiment and analysis attributes generated by both 
researchers and data processing software.
"""

for record in records:
    meta_path = record + "/" + args.json_file[0]
    metaf = s3.open(meta_path, "rb")
    metadata = json.load(metaf)

    # Find the zip file or files
    zipf = s3.glob(record + "/*.zip")

    description_string = f"Files available via S3 at {endpoint}{path}<br>"
    for link in zipf:
        fname = link.split("/")[-1]
        link = endpoint + link
        description_string += f"""{fname} <a class="btn btn-xs piwik_download" 
        type="application/octet-stream" href="{link}">
        <i class="fa fa-download"></i> Download</a>    <br>"""

    metadata["descriptions"].append(
        {"description": description_string, "descriptionType": "Other"},
    )
    metadata["descriptions"].append(
        {"description": abstract, "descriptionType": "Abstract"}
    )

    metadata["types"] = {"resourceType": "", "resourceTypeGeneral": "Dataset"}
    metadata["schemaVersion"] = "http://datacite.org/schema/kernel-4"
    metadata["publicationYear"] = str(metadata["publicationYear"])
    metadata["rightsList"] = [
        {
            "rights": "cc-by-sa-4.0",
            "rightsUri": "http://creativecommons.org/licenses/by-sa/4.0/",
        }
    ]
    metadata["relatedIdentifiers"] = [
        {
            "relatedIdentifier": "10.25989/es8t-kswe",
            "relationType": "IsPartOf",
            "relatedIdentifierType": "DOI",
        },
        {
            "relatedIdentifier": "10.1038/s41524-019-0216-x",
            "relationType": "IsDocumentedBy",
            "relatedIdentifierType": "DOI",
        },
    ]
    metadata["fundingReferences"] = [
        {
            "funderName": "Office of Science of the U.S. Department of Energy",
            "awardNumber": "DE-SC0004993",
        }
    ]

    for meta in metadata.copy():
        if metadata[meta] == []:
            metadata.pop(meta)
    for contributor in metadata["contributors"]:
        if contributor["affiliation"] == []:
            contributor.pop("affiliation")
    for creator in metadata["creators"]:
        if creator["affiliation"] == []:
            creator.pop("affiliation")

    unnecessary = [
        "id",
        "doi",
        "container",
        "providerId",
        "clientId",
        "agency",
        "state",
    ]
    for un in unnecessary:
        metadata.pop(un)
    valid = schema43.validate(metadata)
    if not valid:
        v = schema43.validator.validate(metadata)
        errors = sorted(v.iter_errors(instance), key=lambda e: e.path)
        for error in errors:
            print(error.message)
        exit()

    production = False

    response = caltechdata_write(metadata, token, [], production, "43")
    print(response)
